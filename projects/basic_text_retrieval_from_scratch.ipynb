{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "import tarfile\n",
    "\n",
    "DATA_DIR = '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract_data(url, data_dir=DATA_DIR):\n",
    "    # Check if data folder exists\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "\n",
    "    data = requests.get(url)\n",
    "    with open(f'{data_dir}/cisi.tar.gz', 'wb') as f:\n",
    "        f.write(data.content)\n",
    "\n",
    "    # Extract the data\n",
    "    tar = tarfile.open(f'{data_dir}/cisi.tar.gz')\n",
    "    tar.extractall(f'{data_dir}/cisi')\n",
    "    tar.close()\n",
    "\n",
    "    # Delete the .tar.gz file\n",
    "    os.remove(f'{data_dir}/cisi.tar.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data .tar.gz and store in the data folder\n",
    "# https://ir.dcs.gla.ac.uk/resources/test_collections/cisi/cisi.tar.gz\n",
    "download_and_extract_data('https://ir.dcs.gla.ac.uk/resources/test_collections/cisi/cisi.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def load_documents(filename, schema_mapping):\n",
    "    documents = []\n",
    "    with open(filename, 'r') as file:\n",
    "        content = file.read()\n",
    "        current_doc = {}\n",
    "\n",
    "        for i, line in enumerate(content.split('\\n')):\n",
    "            pattern = re.compile(r'\\.\\w+')\n",
    "            isKey = pattern.match(line)\n",
    "            if isKey:\n",
    "                raw_key = isKey.group()\n",
    "                if raw_key in schema_mapping:\n",
    "                    key_config = schema_mapping[raw_key].copy()\n",
    "                    if 'default' not in key_config:\n",
    "                        key_config[\"default\"] = \"\"\n",
    "                    if 'delimiter' not in key_config:\n",
    "                        key_config[\"delimiter\"] = None\n",
    "                    if key_config[\"alias\"] == 'id':\n",
    "                        if current_doc:\n",
    "                            documents.append(current_doc)\n",
    "                        current_doc = {}\n",
    "                        current_doc[key_config[\"alias\"]] = int(line.split(' ')[1])\n",
    "                    else:\n",
    "                        current_doc[key_config[\"alias\"]] = key_config[\"default\"][:] # Requires [:] to copy the list\n",
    "                else:\n",
    "                    key_config = None\n",
    "            elif key_config is not None:\n",
    "                if key_config[\"default\"] == []:\n",
    "                    if key_config[\"delimiter\"]:\n",
    "                        line = line.split(key_config[\"delimiter\"])\n",
    "                    current_doc[key_config[\"alias\"]].append(line)\n",
    "                else:\n",
    "                    current_doc[key_config[\"alias\"]] += line.strip() + ' '\n",
    "\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_mapping = {\n",
    "    '.I': {\n",
    "        \"alias\": 'id',\n",
    "        \"default\": \"\"\n",
    "    },\n",
    "    '.T': {\n",
    "        \"alias\": 'title',\n",
    "        \"default\": \"\"\n",
    "    },\n",
    "    '.A': {\n",
    "        \"alias\": 'author',\n",
    "        \"default\": []\n",
    "    },\n",
    "    '.W': {\n",
    "        \"alias\": 'abstract',\n",
    "        \"default\": \"\"\n",
    "    },\n",
    "    '.X': {\n",
    "        \"alias\": 'xrefs',\n",
    "        \"default\": [],\n",
    "        \"delimiter\": \"\\t\"\n",
    "    }\n",
    "}\n",
    "docs = load_documents(f'{DATA_DIR}/cisi/CISI.ALL', doc_mapping) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_mapping = {\n",
    "    '.I': {\n",
    "        \"alias\": 'id',\n",
    "        \"default\": \"\"\n",
    "    },\n",
    "    '.W': {\n",
    "        \"alias\": 'query',\n",
    "        \"default\": \"\"\n",
    "    }\n",
    "}\n",
    "queries = load_documents(f'{DATA_DIR}/cisi/CISI.QRY', query_mapping) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results are in the form of <query_id> <document_id> ordered by relevance\n",
    "relevant_results = np.loadtxt(f'{DATA_DIR}/cisi/CISI.REL')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Vector Space Model Implementation\n",
    "\n",
    "A Vector Space model is a Text Retrieval system. We have preselected a Vector Space Model to solve this text retrieval task which means we have already made a few decisions before we even are getting started. A Vector Space Model has the following characteristics to solve TR tasks:\n",
    "- The relevant documents are ranked by relative relevance to the query rather than being classified as relevant or non-relevant.\n",
    "- The documents are considered relevant based on the relative similarity of the query and the document.\n",
    "- The documents are represented as vectors in a multi-dimensional space. The query is also represented as a vector in the same space.\n",
    "\n",
    "The factors that we have to consider when implementing a Vector Space Model are:\n",
    "1. The similarity measure to compare the query and the documents.\n",
    "2. The dimensions to span the vector space.\n",
    "3. The encoding of the documents and the query as vectors.\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "Important properties of a good search are relevance and speed. We will evaluate the performance of our Vector Space Model based on the following metrics:\n",
    "\n",
    "In terms of relevance:\n",
    "   - Mean Average Precision (MAP)\n",
    "\n",
    "In terms of speed:\n",
    "    - Average Query Computational Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the Timing Function\n",
    "import time\n",
    "\n",
    "def time_this(iter, func, *args, **kwargs):\n",
    "  start_time = time.time()\n",
    "\n",
    "  # Your code or function here\n",
    "  # for example:\n",
    "  for i in range(iter):\n",
    "      func(*args, **kwargs)\n",
    "\n",
    "  end_time = time.time()\n",
    "  avg_time = round((end_time - start_time) / iter,6)\n",
    "  print(f\"Avg. Time taken for {iter} iterations:\", avg_time, \"seconds\")\n",
    "  return avg_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_at_k(k, results, relevant_docs):\n",
    "    # k cannot be larger than the number of results\n",
    "    assert k <= len(results)\n",
    "\n",
    "    # Get the top k results\n",
    "    top_k_results = results[:k]\n",
    "    # Get the number of relevant documents in the top k results\n",
    "    num_rel_docs = len(set(top_k_results).intersection(relevant_docs))\n",
    "    # k result is relevant or not \n",
    "    k_is_relevant = is_relevant(results[k-1], relevant_docs)\n",
    "\n",
    "    # Return the precision at k\n",
    "    assert num_rel_docs <= k\n",
    "\n",
    "    return num_rel_docs / k, num_rel_docs / len(relevant_docs), k_is_relevant\n",
    "\n",
    "def is_relevant(doc_id, relevant_results):\n",
    "    # Check if the document is in the relevant results\n",
    "    return doc_id in relevant_results\n",
    "\n",
    "def average_precision_to_k(k,results, relevant_docs):\n",
    "    # For results larger than k we assume that the precision is 0\n",
    "    ap_num = 0\n",
    "    for i in range(1,k+1):\n",
    "        p_i, _, k_is_relevant = precision_recall_at_k(i, results, relevant_docs)\n",
    "        if k_is_relevant:\n",
    "            # If the recall increased, add the precision at k to the AP otherwise add 0\n",
    "            ap_num += p_i\n",
    "    total_rel_docs = len(relevant_docs)\n",
    "\n",
    "    return ap_num / total_rel_docs if total_rel_docs > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We're using Numpy for the implementation as it is much faster than Python lists\n",
    "# list_1 = list(range(1000000))\n",
    "# list_2 = list(range(1000000))\n",
    "\n",
    "# np_1 = np.array(list_1)\n",
    "# np_2 = np.array(list_2)\n",
    "\n",
    "# def add_lists(list_1, list_2):\n",
    "#     return [x + y for x, y in zip(list_1, list_2)]\n",
    "\n",
    "# def add_numpy_arrays(list_1, list_2):\n",
    "#     return np.add(list_1, list_2)\n",
    "\n",
    "# list_perf = time_this(100, add_lists, list_1, list_2)\n",
    "# np_perf = time_this(100, add_numpy_arrays, np_1, np_2)\n",
    "\n",
    "# print(\"Numpy Operations can be\", round(list_perf / np_perf, 2), \"times faster than Python list operations\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Iteration\n",
    "\n",
    "For the first implementation we are going to be using the following:\n",
    "1. Dot-Product as the similarity measure.\n",
    "2. The dimensions / tokens are going to be the unique words in the corpus. \n",
    "3. The encoding of the documents and the query are going to be bit vectors (i.e. a word being present or not).\n",
    "\n",
    "To solve this task we will have to implement the following equation in our ranking function:\n",
    "\n",
    "$$f(d,q) = \\sum^N_{i=1} x_i y_i = \\sum_{w\\ \\in\\ q \\cap d} b(w, q) b(w,d), \\ where \\ b(w,q),b(w,d) \\in \\{0,1\\}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def basic_tokenizer(text):\n",
    "    # Each document is a string of words separated by spaces (does not apply to all languages)\n",
    "    # Remove punctuation and symbols.\n",
    "    pattern = re.compile(r'[^\\w\\s]')\n",
    "    text = pattern.sub('', text)\n",
    "\n",
    "    # Split by spaces and remove empty strings. \n",
    "    # Ensure all words are lowercase and return only unique words.\n",
    "    return set([word.lower() for word in text.split(' ') if word != ''])\n",
    "\n",
    "def basic_inverted_index(tokenized_docs):\n",
    "    # Create the defaultdict\n",
    "    inv_index = defaultdict(dict)\n",
    "\n",
    "    # Loop through the documents\n",
    "    for doc_id, doc in tokenized_docs.items():\n",
    "        for token in doc:\n",
    "            if \"doc_ids\" not in inv_index[token]:\n",
    "                inv_index[token][\"doc_ids\"] = []\n",
    "            if doc_id not in inv_index[token]['doc_ids']:\n",
    "                inv_index[token]['doc_ids'].append(doc_id)\n",
    "            \n",
    "    return inv_index\n",
    "\n",
    "def basic_ranking_function(query, inv_index):\n",
    "    # Implementation of a basic ranking function\n",
    "    # Takes a query and an inverted index and returns a ranked list of documents\n",
    "    # according to the basic scoring function\n",
    "    tokenized_query = basic_tokenizer(query)\n",
    "\n",
    "    scores = defaultdict(int)\n",
    "    for word in tokenized_query:\n",
    "        for doc_id in inv_index[word]['doc_ids']:\n",
    "            scores[doc_id] += 1\n",
    "    return sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "def basic_search(query, inv_index, docs_by_id):\n",
    "    # Implementation of a basic search\n",
    "    # Takes a query and an inverted index and returns a ranked list of documents\n",
    "\n",
    "    ranks = basic_ranking_function(query, inv_index)\n",
    "    return [(docs_by_id[doc_id], score) for doc_id, score in ranks]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_by_id = {doc['id']: doc for doc in docs}\n",
    "\n",
    "# Preprocess the documents (add title and abstract) into a dictionary by ID\n",
    "prepared_docs = {doc['id']: doc['title'] + ' ' + doc['abstract'] for doc in docs}\n",
    "tokenized_docs = {doc_id: basic_tokenizer(doc) for doc_id, doc in prepared_docs.items()}\n",
    "\n",
    "# Create the inverted index\n",
    "inv_index = basic_inverted_index(tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Time taken for 1000 iterations: 0.001169 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.001169"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_this(1000, basic_search, queries[0]['query'], inv_index, docs_by_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What problems and concerns are there in making up descriptive titles? What difficulties are involved in automatically retrieving articles from approximate titles? What is the usual relevance of the content of articles to their titles? \n",
      "ID: 1054\n",
      "Title: Relevance Predictability in Information Retrieval Systems \n",
      "Snippet: An experiment is described which attempts to derive quantitative indicators regarding the potential ...\n",
      "Score: 12\n",
      "ID: 1124\n",
      "Title: Some Aspects of Developing and Studying a Descriptor Information Language for General Technology \n",
      "Snippet: The methods and results of an endeavor to develop an information retrieval language for automatic re...\n",
      "Score: 12\n",
      "ID: 28\n",
      "Title: A Note on the Pseudo-Mathematics of Relevance \n",
      "Snippet: Recently a number of articles, books, and reports dealing with information systems, i.e., document r...\n",
      "Score: 12\n"
     ]
    }
   ],
   "source": [
    "sample_query = queries[0]['query']\n",
    "print(\"Query: \" + sample_query)\n",
    "results = basic_search(sample_query, inv_index, docs_by_id)\n",
    "for result in results[:3]:\n",
    "    print(\"ID: \" + str(result[0][\"id\"]))\n",
    "    print(\"Title: \" + result[0][\"title\"])\n",
    "    print(\"Snippet: \" + result[0][\"abstract\"][:100] + \"...\")\n",
    "    print(\"Score: \" + str(result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_relevant_results = relevant_results[relevant_results[:, 0] == 1][:, 1].astype(int)\n",
    "result_ids = [result[0]['id'] for result in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision at 1000 0.06496935449439897\n"
     ]
    }
   ],
   "source": [
    "k = 1000\n",
    "print(f\"Average Precision at {k}\", average_precision_to_k(k,result_ids, sample_relevant_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "small_projects",
   "language": "python",
   "name": "small_projects"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
